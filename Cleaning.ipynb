{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6149f1cd",
   "metadata": {},
   "source": [
    "DESCRIPTION: \n",
    "Cleaning.ipynb is focused on preparing the raw dataset for analysis. It imports the raw files, handles issues like missing values, duplicates, and inconsistent entries, and standardizes the structure of the data. The notebook likely also filters out irrelevant or erroneous records and organizes the dataset into a clean, consistent format that can be reused. By the end of this notebook, the data is ready for analysis and may be saved into a cleaned CSV or database table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e16e8a",
   "metadata": {},
   "source": [
    "### Dropping Columns that have low quality data and are less important for our EDA\n",
    "### Dropping Repeated Product_id Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading data from 'Assignment\\Data\\Silver\\data_raw.csv'...\n",
      "[ERROR] The file 'Assignment\\Data\\Silver\\data_raw.csv' was not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def drop_unnecessary_columns(input_file: str):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, drops a predefined list of columns, removes duplicate\n",
    "    product_id entries, and returns the result.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): The path to the input CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[*] Reading data from '{input_file}'...\")\n",
    "        # Use low_memory=False for large files to avoid data type warnings\n",
    "        df = pd.read_csv(input_file, low_memory=False)\n",
    "        print(f\"[+] Successfully loaded {len(df)} rows.\")\n",
    "\n",
    "        # --- List of columns to be removed ---\n",
    "        columns_to_drop = [\n",
    "            'last_verified',\n",
    "            'payment_terms',\n",
    "            'delivery_time',  \n",
    "            'minimum_order_quantity', \n",
    "            'supply_ability',\n",
    "            'trust_stamp_url',\n",
    "            'certifications',\n",
    "            'made_in_india',\n",
    "        ]\n",
    "        \n",
    "        # Check which of the columns to drop actually exist in the DataFrame\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        \n",
    "        if not existing_columns_to_drop:\n",
    "            print(\"[INFO] None of the specified columns were found to drop.\")\n",
    "            df_clean = df.copy() # Work with a copy if no columns are dropped\n",
    "        else:\n",
    "            print(f\"[*] Dropping the following columns: {existing_columns_to_drop}\")\n",
    "            # Drop the columns\n",
    "            df_clean = df.drop(columns=existing_columns_to_drop)\n",
    "            print(f\"    Original number of columns: {len(df.columns)}\")\n",
    "            print(f\"    New number of columns: {len(df_clean.columns)}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # --- Remove Duplicate product_id Entries ---\n",
    "        print(\"[*] Removing duplicate entries based on 'product_id'...\")\n",
    "        rows_before_dedupe = len(df_clean)\n",
    "        print(f\"    Number of rows before deduplication: {rows_before_dedupe}\")\n",
    "\n",
    "        # Drop duplicates, keeping the first occurrence\n",
    "        df_deduplicated = df_clean.drop_duplicates(subset=['product_id'], keep='first')\n",
    "        \n",
    "        rows_after_dedupe = len(df_deduplicated)\n",
    "        print(f\"    Number of rows after deduplication: {rows_after_dedupe}\")\n",
    "        \n",
    "        duplicates_removed = rows_before_dedupe - rows_after_dedupe\n",
    "        print(f\"[+] Total duplicate product_id entries removed: {duplicates_removed}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(\"data carried with df_cleaned_up\")\n",
    "        return df_deduplicated\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file '{input_file}' was not found.\")\n",
    "    except KeyError:\n",
    "        print(\"Error: The column 'product_id' was not found in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your input file here\n",
    "    input_filename = r'C:\\Users\\yuvra\\OneDrive\\Desktop\\Slooze-Assignment\\Assignment\\Data\\Silver\\data_raw.csv'\n",
    "    \n",
    "    \n",
    "    # Run the function\n",
    "    df_cleaned_up = drop_unnecessary_columns(input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf197",
   "metadata": {},
   "source": [
    "# corrected DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25e60a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading data from 'df_cleaned'\n",
      "[*] Converting data types for memory and performance optimization...\n",
      "\n",
      "--- Data Types After Conversion ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 359964 entries, 0 to 367219\n",
      "Data columns (total 31 columns):\n",
      " #   Column                Non-Null Count   Dtype   \n",
      "---  ------                --------------   -----   \n",
      " 0   product_id            359964 non-null  object  \n",
      " 1   profile_id            359964 non-null  object  \n",
      " 2   userid                359964 non-null  object  \n",
      " 3   product_name          359962 non-null  object  \n",
      " 4   co_name               359959 non-null  object  \n",
      " 5   prod_url              359964 non-null  object  \n",
      " 6   profile_url           359964 non-null  object  \n",
      " 7   main_category         359964 non-null  category\n",
      " 8   sub_category          359964 non-null  category\n",
      " 9   member_since          37481 non-null   Int64   \n",
      " 10  year_established      269224 non-null  Int64   \n",
      " 11  has_trust_stamp       359964 non-null  boolean \n",
      " 12  city                  359838 non-null  category\n",
      " 13  state                 359850 non-null  category\n",
      " 14  country_name          359920 non-null  category\n",
      " 15  is_manufacturer       359964 non-null  boolean \n",
      " 16  is_distributor        359964 non-null  boolean \n",
      " 17  is_supplier           359964 non-null  boolean \n",
      " 18  is_exporter           359964 non-null  boolean \n",
      " 19  is_trader             359964 non-null  boolean \n",
      " 20  is_service_provider   359964 non-null  boolean \n",
      " 21  price_string          116561 non-null  object  \n",
      " 22  price_numeric         359921 non-null  float64 \n",
      " 23  price_range           12305 non-null   object  \n",
      " 24  min_price_range       359919 non-null  float64 \n",
      " 25  max_price_range       359919 non-null  float64 \n",
      " 26  product_description   359612 non-null  object  \n",
      " 27  keywords              321914 non-null  object  \n",
      " 28  price_from_sub_json   143117 non-null  object  \n",
      " 29  buyer_feedback_score  169550 non-null  Int64   \n",
      " 30  in_stock              359911 non-null  boolean \n",
      "dtypes: Int64(3), boolean(8), category(5), float64(3), object(12)\n",
      "memory usage: 61.7+ MB\n",
      "data carried by df_corrected\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def format_data_types():\n",
    "    \"\"\"\n",
    "    Reads a data file, converts columns to optimal data types based on specific\n",
    "    rules, and saves the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[*] Reading data from 'df_cleaned'\")\n",
    "        \n",
    "        # --- Define columns for each data type category based on your rules ---\n",
    "        \n",
    "        # Rule 1: Identifiers should be strings\n",
    "        string_id_cols = ['product_id', 'profile_id', 'userid']\n",
    "        \n",
    "        # Rule 2: Price columns should be floats, others should be nullable integers\n",
    "        float_cols = ['min_price', 'max_price', 'fix_price']\n",
    "        integer_cols = ['member_since', 'year_established', 'buyer_feedback_score']\n",
    "        \n",
    "        # Rule 3: Boolean columns\n",
    "        boolean_cols = [\n",
    "            'has_trust_stamp', 'is_distributor', 'is_supplier', 'is_exporter',\n",
    "            'is_trader', 'is_service_provider', 'in_stock', 'is_manufacturer'\n",
    "        ]\n",
    "        \n",
    "        # Rule 4: Categorical columns for memory efficiency\n",
    "        category_cols = [\n",
    "            'main_category', 'sub_category', 'city', 'state', 'country_name'\n",
    "        ]\n",
    "\n",
    "        print(\"[*] Converting data types for memory and performance optimization...\")\n",
    "\n",
    "        # --- Apply Conversions ---\n",
    "        for col in string_id_cols:\n",
    "            if col in df_cleaned_up.columns:\n",
    "                df_cleaned_up[col] = df_cleaned_up[col].astype(str)\n",
    "\n",
    "        for col in float_cols:\n",
    "            if col in df_cleaned_up.columns:\n",
    "                df_cleaned_up[col] = pd.to_numeric(df_cleaned_up[col], errors='coerce')\n",
    "\n",
    "        for col in integer_cols:\n",
    "            if col in df_cleaned_up.columns:\n",
    "                # First convert to numeric, coercing errors to NaN\n",
    "                numeric_series = pd.to_numeric(df_cleaned_up[col], errors='coerce')\n",
    "                # Then convert to nullable integer type 'Int64'\n",
    "                df_cleaned_up[col] = numeric_series.astype('Int64')\n",
    "\n",
    "        for col in boolean_cols:\n",
    "            if col in df_cleaned_up.columns:\n",
    "                df_cleaned_up[col] = df_cleaned_up[col].astype('boolean')\n",
    "\n",
    "        for col in category_cols:\n",
    "            if col in df_cleaned_up.columns:\n",
    "                df_cleaned_up[col] = df_cleaned_up[col].astype('category')\n",
    "        \n",
    "        # --- Final Report and Save ---\n",
    "        print(\"\\n--- Data Types After Conversion ---\")\n",
    "        df_cleaned_up.info(verbose=True, show_counts=True)\n",
    "        print('data carried by df_corrected')\n",
    "        return df_cleaned_up\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file 'df_dropped' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    # Run the main function\n",
    "    df_corrected = format_data_types()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e3317",
   "metadata": {},
   "source": [
    "### Price cleaning script Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ee24262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting price processing pipeline...\n",
      "[*] Removed original price columns for a cleaner final dataset.\n",
      "[*] Ensured price columns are numeric for analysis.\n",
      "[+] Price processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "USD_TO_INR_RATE = 87.27\n",
    "\n",
    "# --- Helper functions (parse_price_string, process_price_columns, cleanup_price_columns) ---\n",
    "# (These functions from your original script are assumed to be here and are correct)\n",
    "\n",
    "def parse_price_string(price_input):\n",
    "    \"\"\"\n",
    "    Parses a single string or number to extract price information.\n",
    "    \"\"\"\n",
    "    if pd.isna(price_input) or not isinstance(price_input, str):\n",
    "        if isinstance(price_input, (int, float)) and price_input > 0:\n",
    "            return None, None, float(price_input)\n",
    "        return None, None, None\n",
    "    price_str = str(price_input).strip()\n",
    "    multiplier = USD_TO_INR_RATE if '$' in price_str else 1.0\n",
    "    found_patterns = re.findall(r'[\\d,.]+', price_str)\n",
    "    numbers = []\n",
    "    for pattern in found_patterns:\n",
    "        try:\n",
    "            num = float(pattern.replace(',', ''))\n",
    "            numbers.append(num)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if not numbers:\n",
    "        return None, None, None\n",
    "    if '-' in price_str or 'to' in price_str.lower():\n",
    "        if len(numbers) >= 2:\n",
    "            min_p = min(numbers) * multiplier\n",
    "            max_p = max(numbers) * multiplier\n",
    "            return min_p, max_p, None\n",
    "    if len(numbers) == 1:\n",
    "        fix_p = numbers[0] * multiplier\n",
    "        return None, None, fix_p\n",
    "    return None, None, None\n",
    "\n",
    "def process_price_columns(df):\n",
    "    \"\"\"\n",
    "    Applies the full price extraction logic to the DataFrame.\n",
    "    \"\"\"\n",
    "    def get_price_for_row(row):\n",
    "        min_r = row.get('min_price_range', 0)\n",
    "        max_r = row.get('max_price_range', 0)\n",
    "        # Ensure min_r and max_r are numeric before comparison\n",
    "        if pd.api.types.is_number(min_r) and pd.api.types.is_number(max_r):\n",
    "            if min_r > 0 and max_r > 0 and max_r >= min_r:\n",
    "                return min_r, max_r, np.nan\n",
    "        \n",
    "        priority_columns = ['price_string', 'price_numeric', 'price_from_sub_json']\n",
    "        for col in priority_columns:\n",
    "            price_val = row.get(col)\n",
    "            if not pd.isna(price_val):\n",
    "                min_p, max_p, fix_p = parse_price_string(price_val)\n",
    "                if min_p is not None or fix_p is not None:\n",
    "                    return min_p, max_p, fix_p\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    df[['min_price', 'max_price', 'fix_price']] = df.apply(get_price_for_row, axis=1, result_type='expand')\n",
    "    return df\n",
    "\n",
    "def cleanup_price_columns(df):\n",
    "    \"\"\"\n",
    "    Removes the original, raw price columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    columns_to_drop = [\n",
    "        'price_string', 'price_numeric', 'price_range',\n",
    "        'min_price_range', 'max_price_range', 'price_from_sub_json'\n",
    "    ]\n",
    "    df_cleaned_price = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    print(\"[*] Removed original price columns for a cleaner final dataset.\")\n",
    "    return df_cleaned_price\n",
    "\n",
    "\n",
    "# --- REVISED FUNCTION ---\n",
    "def format_price_columns_for_analysis(df):\n",
    "    \"\"\"\n",
    "    Ensures price columns are purely numeric for analysis.\n",
    "    \"\"\"\n",
    "    # 1. Ensure price columns are purely numeric. Any non-numeric becomes NaN.\n",
    "    df['min_price'] = pd.to_numeric(df['min_price'], errors='coerce')\n",
    "    df['max_price'] = pd.to_numeric(df['max_price'], errors='coerce')\n",
    "    df['fix_price'] = pd.to_numeric(df['fix_price'], errors='coerce')\n",
    "    \n",
    "    print(\"[*] Ensured price columns are numeric for analysis.\")\n",
    "    return df\n",
    "\n",
    "# --- Main processing function that uses the REVISED formatting function ---\n",
    "def run_price_pipeline(input_df):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame, runs the full price cleaning and formatting pipeline,\n",
    "    and returns the final processed DataFrame.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_df, pd.DataFrame):\n",
    "        print(\"[ERROR] Input must be a pandas DataFrame.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"[*] Starting price processing pipeline...\")\n",
    "    # Make a copy to avoid changing the original DataFrame\n",
    "    df_processed = process_price_columns(input_df.copy()) \n",
    "    df_cleaned_price = cleanup_price_columns(df_processed)\n",
    "    \n",
    "    # Use the new, analysis-friendly formatting function\n",
    "    df_final = format_price_columns_for_analysis(df_cleaned_price)\n",
    "    \n",
    "    print(\"[+] Price processing complete.\")\n",
    "    return df_final\n",
    "\n",
    "# --- Example Main execution block for your notebook ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    df_price_corrected = run_price_pipeline(df_corrected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d918b98",
   "metadata": {},
   "source": [
    "## Tried Cleaning yr_established (not very effective, % did not change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f71ba788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Filled missing 'year_established' values and set correct data type.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def fill_year_established(input_df):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame, fills missing 'year_established' values based on \n",
    "    the 'member_since' column, and returns the modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Work on a copy to ensure the original DataFrame is not changed\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # --- 1. Data Preparation ---\n",
    "    # Ensure both columns are numeric, converting non-numbers to NaN (missing).\n",
    "    df['member_since'] = pd.to_numeric(df['member_since'], errors='coerce')\n",
    "    df['year_established'] = pd.to_numeric(df['year_established'], errors='coerce')\n",
    "\n",
    "    # --- 2. Imputation Logic ---\n",
    "    # Calculate the estimated year of establishment where possible.\n",
    "    estimated_year = 2025 - df['member_since']\n",
    "\n",
    "    # Use .fillna() to fill ONLY the missing 'year_established' values.\n",
    "    # This now assigns the result back to the column, avoiding the warning.\n",
    "    df['year_established'] = df['year_established'].fillna(estimated_year)\n",
    "\n",
    "    # --- 3. Final Data Type Conversion ---\n",
    "    # Convert the column to the nullable integer type 'Int64'.\n",
    "    # Any remaining missing values will be correctly stored as pd.NA.\n",
    "    df['year_established'] = df['year_established'].astype('Int64')\n",
    "    \n",
    "    print(\"[*] Filled missing 'year_established' values and set correct data type.\")\n",
    "    \n",
    "    # Return the fully processed DataFrame\n",
    "    return df\n",
    "\n",
    "# --- Main execution block for your notebook ---\n",
    "# This demonstrates how to use the corrected function.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Call the main function with your DataFrame\n",
    "df_yr = fill_year_established(df_price_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8e03e",
   "metadata": {},
   "source": [
    "### Cleaning states and country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4b00d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique states before cleaning: 38\n",
      "Number of unique states after cleaning: 35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define the Comprehensive Cleaning Rules ---\n",
    "# This dictionary is built directly from the list of unique values you provided.\n",
    "state_mapping = {\n",
    "    # Handle case variations\n",
    "    'karnataka': 'Karnataka',\n",
    "    \n",
    "    # Handle old vs. new names\n",
    "    'Pondicherry': 'Puducherry',\n",
    "    \n",
    "    # Handle merged territories\n",
    "    'Dadra and Nagar Haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "    'Daman and Diu': 'Dadra and Nagar Haveli and Daman and Diu'\n",
    "}\n",
    "\n",
    "# --- 2. Apply the Cleaning ---\n",
    "# Filter for India to get the 'before' count\n",
    "india_df = df_price_corrected[df_price_corrected['country_name'] == 'India']\n",
    "before_count = india_df['state'].nunique()\n",
    "print(f\"Number of unique states before cleaning: {before_count}\")\n",
    "\n",
    "# Apply the mapping to the entire 'state' column in the main DataFrame.\n",
    "# We convert the column to a string type first to avoid the FutureWarning.\n",
    "df_price_corrected['state_cleaned'] = df_price_corrected['state'].astype(str).replace(state_mapping)\n",
    "\n",
    "# --- 3. Verify the Result ---\n",
    "# Filter for India again, but use the new 'state_cleaned' column\n",
    "india_df_cleaned = df_price_corrected[df_price_corrected['country_name'] == 'India']\n",
    "after_count = india_df_cleaned['state_cleaned'].nunique()\n",
    "print(f\"Number of unique states after cleaning: {after_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0a3bb",
   "metadata": {},
   "source": [
    "### tells % of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0b58030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Analyzing DataFrame with 359964 rows...\n",
      "[*] Custom missing values ('Not available', '-', '0', etc.) have been handled.\n",
      "\n",
      "============================================================\n",
      "--- Column Data Completeness Report ---\n",
      "Column Name                              | % Available Data\n",
      "------------------------------------------------------------\n",
      "product_id                               |          100.00%\n",
      "profile_id                               |          100.00%\n",
      "userid                                   |          100.00%\n",
      "prod_url                                 |          100.00%\n",
      "has_trust_stamp                          |          100.00%\n",
      "profile_url                              |          100.00%\n",
      "main_category                            |          100.00%\n",
      "sub_category                             |          100.00%\n",
      "is_distributor                           |          100.00%\n",
      "is_supplier                              |          100.00%\n",
      "is_exporter                              |          100.00%\n",
      "is_manufacturer                          |          100.00%\n",
      "is_trader                                |          100.00%\n",
      "is_service_provider                      |          100.00%\n",
      "product_name                             |          100.00%\n",
      "co_name                                  |          100.00%\n",
      "country_name                             |           99.99%\n",
      "in_stock                                 |           99.99%\n",
      "state                                    |           99.97%\n",
      "city                                     |           99.96%\n",
      "product_description                      |           99.90%\n",
      "keywords                                 |           89.43%\n",
      "year_established                         |           74.82%\n",
      "buyer_feedback_score                     |           39.22%\n",
      "fix_price                                |           31.73%\n",
      "member_since                             |           10.40%\n",
      "max_price                                |            5.68%\n",
      "min_price                                |            5.68%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_completeness(df):\n",
    "    \"\"\"\n",
    "    Calculates and prints the percentage of available (non-null) data \n",
    "    for each column in a pandas DataFrame after handling custom missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to analyze.\n",
    "    \"\"\"\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"[ERROR] Input must be a pandas DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[INFO] The DataFrame is empty. No analysis to perform.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[*] Analyzing DataFrame with {len(df)} rows...\")\n",
    "\n",
    "    try:\n",
    "        # --- Data Pre-processing ---\n",
    "        # Create a copy to avoid modifying the original DataFrame sent to the function.\n",
    "        df_cleaned = df.copy()\n",
    "\n",
    "        # Define a list of values that should be treated as missing.\n",
    "        values_to_replace = ['Not available','Not available','Negotiable', '-', 'unavailable', 'N/A', 0, '0']\n",
    "        \n",
    "        # Replace all specified values with NumPy's Not a Number (NaN).\n",
    "        df_cleaned.replace(values_to_replace, np.nan, inplace=True)\n",
    "        \n",
    "        print(\"[*] Custom missing values ('Not available', '-', '0', etc.) have been handled.\")\n",
    "\n",
    "        # --- Core Calculation Logic ---\n",
    "        # 1. Get the total number of rows (the denominator for our percentage).\n",
    "        total_rows = len(df_cleaned)\n",
    "\n",
    "        # 2. Count non-null values in each column of the cleaned data.\n",
    "        non_null_counts = df_cleaned.count()\n",
    "\n",
    "        # 3. Calculate the percentage of data available.\n",
    "        #    This is the heart of the analysis.\n",
    "        completeness_report = (non_null_counts / total_rows) * 100\n",
    "\n",
    "        # 4. Sort the results for a more insightful report (most complete first).\n",
    "        sorted_report = completeness_report.sort_values(ascending=False)\n",
    "\n",
    "        # --- Display the Final Report ---\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"--- Column Data Completeness Report ---\")\n",
    "        print(f\"{'Column Name':<40} | {'% Available Data'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Loop through the sorted series to print each column's completeness\n",
    "        for column_name, percentage in sorted_report.items():\n",
    "            print(f\"{column_name:<40} | {percentage:>15.2f}%\")\n",
    "            \n",
    "        print(\"=\"*60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred during analysis: {e}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Call the function to analyze your DataFrame\n",
    "    calculate_completeness(df_yr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5354b",
   "metadata": {},
   "source": [
    "# saving to Parquet, Because parquet store column Dtypes and they are faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66055d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yr.to_csv(r'C:\\Users\\yuvra\\OneDrive\\Desktop\\Slooze-Assignment\\Assignment\\Data\\Gold\\Data_processed.csv', index=False)\n",
    "df_yr.to_parquet(r'C:\\Users\\yuvra\\OneDrive\\Desktop\\Slooze-Assignment\\Assignment\\Data\\Gold\\Data_processed.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
